# config.yaml


## samples path
save_data: run

# Training files
data:
    corpus_1:
        path_src: train.source
        path_tgt: train.target
        transforms: [filtertoolong]     # Not sure if needed on character-level

train_from: models/ocr.pt       # Path to state_dict, to use pre-trained model
reset_optim: 'states'             # Default: 'none', required 'states' or 'all' when using update_vocab

# Vocabulary files, generated by onmt_build_vocab
src_vocab: run/source.vocab
tgt_vocab: run/target.vocab

update_vocab: 'true'            # Merge NATAS vocab with our

# Vocabulary size
src_vocab_size: 500
tgt_vocab_size: 500

# Filter out source/target longer than n if [filtertoolong] enabled
src_seq_length: 200
tgt_seq_length: 200

# Where to save the log file and the output models/checkpoints
log_file: train.log
save_model: models/model.fren

# Stop training if it does not improve after n validations
early_stopping: 4

# Default: 5000 - Save a model checkpoint for each n
save_checkpoint_steps: 1000

# To save space, limit checkpoints to last n
keep_checkpoint: 3

seed: 1242

# Default: 100000 - Train the model to max n steps
train_steps: 150000         # train_steps is absolute, not relative for train_from (add steps from checkpoint)

# Default: 4000 - for large datasets, try up to 8000
warmup_steps: 500
report_every: 100

decoder_type: transformer
encoder_type: brnn          # Used in the NATAS paper
word_vec_size: 512          # Default, maybe lower for character-level ?
rnn_size: 512               # Default
layers: 2                   # Used in the NATAS paper
transformer_ff: 2048        # Default
heads: 8                    # Default

accum_count: 4              # Recommanded for transformer, maybe change
optim: sgd                  # Default
decay_method: noam          # Some custom decay rate
learning_rate: 1.0          # Default for sgd
max_grad_norm: 5.0          # Default

# Tokens per batch, change if out of GPU memory
batch_size: 4096            # Higher the better ;)
#valid_batch_size: 4096      # Higher the better ;)
batch_type: tokens          # Dynamic batching
normalization: tokens
dropout: 0.3                # Default
label_smoothing: 0.0        # Default, no smoothing

max_generator_batches: 32   # Default, Higher the better ;)

param_init: 0.0             # No init
param_init_glorot: 'true'   # Required for transformer
position_encoding: 'true'   # Necessary for non-RNN style models

# Number of GPUs, and IDs of GPUs
world_size: 1               # Change before using on cluster
gpu_ranks: [0]              # Change before using on cluster

