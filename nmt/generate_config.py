#!/usr/bin/env python3
# -*- coding: utf-8 -*-

# ----------PARAMETERS SETTING----------

# Add Validation parameters to config file
validation = False
# Add filtering parameters to config file
filtering = True

src_train_name = "jebb_train.source.tokenized"
tgt_train_name = "jebb_train.target.tokenized"

src_valid_name = "jebb_valid.source.tokenized"
tgt_valid_name = "jebb_valid.target.tokenized"

src_vocab_size = "500"
tgt_vocab_size = "500"

num_gpu = "1"
gpu_ranks = "[0]"

config = '''# config.yaml


## samples path
save_data: run

# Training files
data:
    corpus_1:
        path_src: ''' + src_train_name + '''
        path_tgt: ''' + tgt_train_name + '''
        transforms: [filtertoolong]
'''

if validation:
    config = config + '''   valid:
        path_src: ''' + src_valid_name + '''
        path_tgt: ''' + tgt_valid_name + '''
        transforms: [filtertoolong]
        '''

config = config + '''
# Vocabulary files, generated by onmt_build_vocab
src_vocab: run/source.vocab
tgt_vocab: run/target.vocab

# Vocabulary size
src_vocab_size: ''' + src_vocab_size + '''
tgt_vocab_size: ''' + tgt_vocab_size + '''

# Filter out source/target longer than n if [filtertoolong] enabled
src_seq_length: 200
tgt_seq_length: 200

# Where to save the log file and the output models/checkpoints
log_file: train.log
save_model: models/model.fren

# Stop training if it does not improve after n validations
early_stopping: 4

# Default: 5000 - Save a model checkpoint for each n
save_checkpoint_steps: 1000

# To save space, limit checkpoints to last n
keep_checkpoint: 3

seed: 1242

# Default: 100000 - Train the model to max n steps 
# Increase for large datasets
train_steps: 3000
'''

if validation:
    config = config + '''# Default: 10000 - Run validation after n steps
    valid_steps: 1000
    '''

config = config + '''
# Default: 4000 - for large datasets, try up to 8000
warmup_steps: 1000
report_every: 100

decoder_type: transformer
encoder_type: transformer
word_vec_size: 512
rnn_size: 512
layers: 6
transformer_ff: 2048
heads: 8

accum_count: 4
optim: adam
adam_beta1: 0.9
adam_beta2: 0.998
decay_method: noam
learning_rate: 2.0
max_grad_norm: 0.0

# Tokens per batch, change if out of GPU memory
batch_size: 4096
valid_batch_size: 4096
batch_type: tokens
normalization: tokens
dropout: 0.1
label_smoothing: 0.1

max_generator_batches: 2

param_init: 0.0
param_init_glorot: 'true'
position_encoding: 'true'

# Number of GPUs, and IDs of GPUs
world_size: ''' + num_gpu + '''
gpu_ranks: ''' + gpu_ranks + '''

'''

with open("config.yaml", "w+") as config_yaml:
    config_yaml.write(config)
